{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload       \n",
    "%autoreload 2              \n",
    "from news_articles import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the search terms used to seed the 'background' dataset are:\n",
    "\n",
    "    (\"nhs\") AND (\"consultation\")\n",
    "\n",
    "and the search terms used to seed the 'signal' dataset are:\n",
    "\n",
    "    (\"nhs\") AND (\"video consultations\" OR \"skype consultations\" OR \"video consulation\" OR \"skype consultation\" OR \"remote consultation\" OR \"remote consultations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open data files and preprocess for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(label, _tokenize=True, field='content'):\n",
    "    with open(f'{label}_search.json') as f:\n",
    "        _signal = json.load(f)\n",
    "    with open(f'nhs_{label}_search.json') as f:\n",
    "        _background = json.load(f)\n",
    "\n",
    "    signal, background = [], []\n",
    "    signal_titles = set()\n",
    "    for article in _signal:\n",
    "        if article['title'] in signal_titles:\n",
    "            continue\n",
    "        signal_titles.add(article['title'])\n",
    "        signal.append(article)\n",
    "    for article in _background:\n",
    "        if article['title'] in signal_titles:\n",
    "            continue  \n",
    "        background.append(article)\n",
    "        if len(background) == len(signal):\n",
    "            break\n",
    "\n",
    "    if _tokenize:\n",
    "        signal = tokenize(signal, field=field)\n",
    "        background = tokenize(background, field=field)\n",
    "    return signal, background\n",
    "    \n",
    "def save_processed(data, label):\n",
    "    with open(f'processed/{label}.json', 'w') as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "def load_processed(label):\n",
    "    with open(f'processed/{label}.json') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "## Comment out if don't need to re-run\n",
    "#with open('final_large_search.json') as f:\n",
    "#     _final_signal = json.load(f)    \n",
    "#\n",
    "# signal, background = get_data('big')  # Train\n",
    "# valid_signal, valid_background = get_data('small')  # Test\n",
    "# extrap_signal, extrap_background = get_data('very_small')  # Valid\n",
    "# final_signal = tokenize(_final_signal, field='content') # Extrap\n",
    "#\n",
    "# save_processed(signal, 'signal')\n",
    "# save_processed(background, 'background')\n",
    "# save_processed(valid_signal, 'valid_signal')\n",
    "# save_processed(valid_background, 'valid_background')\n",
    "# save_processed(extrap_signal, 'extrap_signal')\n",
    "# save_processed(extrap_background, 'extrap_background')\n",
    "# save_processed(final_signal, 'final_signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = load_processed('signal')\n",
    "background = load_processed('background')\n",
    "valid_signal = load_processed('valid_signal')\n",
    "valid_background = load_processed('valid_background')\n",
    "extrap_signal = load_processed('extrap_signal')\n",
    "extrap_background = load_processed('extrap_background')\n",
    "final_signal = load_processed('final_signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a predictive model for 'remote consultation' articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force optimisers\n",
    "def optimise_preprocessing(signal, background, \n",
    "                           valid_signal, valid_background,\n",
    "                           min_df_range, max_df_range, \n",
    "                           ngram_range_range, n_estimators_range, \n",
    "                           max_dept_range):\n",
    "    best_score = 0\n",
    "    for min_df in np.arange(*min_df_range):\n",
    "        for max_df in np.arange(*max_df_range):\n",
    "            for ngram_range in np.arange(*ngram_range_range):\n",
    "                # Testing set\n",
    "                vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=(1,ngram_range))\n",
    "                X = vectorizer.fit_transform([' '.join(doc) for doc in signal + background])\n",
    "                y = [1]*len(signal) + [0]*len(background)\n",
    "                # Training set\n",
    "                X0 = vectorizer.transform([' '.join(doc) for doc in valid_signal + valid_background])\n",
    "                y0 = [1]*len(valid_signal) + [0]*len(valid_background)        \n",
    "                for n_estimators in np.arange(*n_estimators_range):\n",
    "                    for max_depth in np.arange(*max_dept_range):                \n",
    "                        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=0, min_samples_split=2, )\n",
    "                        clf = clf.fit(X, y)\n",
    "                        score = clf.score(X0, y0)\n",
    "                        #sample_space[n_estimators][max_depth] = score\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            print(min_df, max_df, ngram_range, n_estimators, max_depth, '--->', score)\n",
    "                            \n",
    "\n",
    "def optimise_model(X, y, X0, y0, n_estimators_range, max_dept_range, min_samples_split_range):\n",
    "    best_score = 0\n",
    "    for n_estimators in np.arange(*n_estimators_range):\n",
    "        for max_depth in np.arange(*max_dept_range):                \n",
    "            for min_samples_split in np.arange(*min_samples_split_range):        \n",
    "                clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=0, \n",
    "                                             min_samples_split=min_samples_split, )\n",
    "                clf = clf.fit(X, y)\n",
    "                score = clf.score(X0, y0)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    print(n_estimators, max_depth, min_samples_split, '--->', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Predictive model based on the main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented out when not optimising\n",
    "## Initially try to optimise the preprocessing\n",
    "# optimise_preprocessing(signal, background, valid_signal, valid_background,\n",
    "#                        min_df_range=(2, 10, 2), \n",
    "#                        max_df_range=(0.8, 0.96, 0.05), \n",
    "#                        ngram_range_range=(1,4,1)),\n",
    "#                        n_estimators_range=(5, 206, 20), \n",
    "#                        max_dept_range=(2, 11, 2))\n",
    "\n",
    "# Preprocess the data with discovered parameters\n",
    "vectorizer = TfidfVectorizer(min_df=8, max_df=0.8, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform([' '.join(doc) for doc in signal + background])\n",
    "y = [1]*len(signal) + [0]*len(background)\n",
    "\n",
    "X0 = vectorizer.transform([' '.join(doc) for doc in valid_signal + valid_background])\n",
    "y0 = [1]*len(valid_signal) + [0]*len(valid_background) \n",
    "\n",
    "X1 = vectorizer.transform([' '.join(doc) for doc in extrap_signal + extrap_background])\n",
    "y1 = [1]*len(extrap_signal) + [0]*len(extrap_background) \n",
    "\n",
    "## Commented out when not optimising\n",
    "## Dig a little deeper to optimise the model given the optimal preprocessing\n",
    "# optimise_model(X, y, X0, y0, \n",
    "#                n_estimators_range=(40, 55, 1), \n",
    "#                max_dept_range=(7, 14, 1), \n",
    "#                min_samples_split_range=(2, 14, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825949367088608 0.8669354838709677 0.8133333333333334\n",
      "[[67  8]\n",
      " [20 55]]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=41, max_depth=11, random_state=0, min_samples_split=2, )\n",
    "clf = clf.fit(X, y)\n",
    "cm = confusion_matrix(y1, clf.predict(X1))\n",
    "\n",
    "print(clf.score(X, y), clf.score(X0, y0), clf.score(X1, y1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf = vectorizer.transform([' '.join(doc) for doc in final_signal])\n",
    "probs = [x[1] for x in clf.predict_proba(Xf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8995941022149845\n",
      "Chelsea pays for hospital staff to stay in club hotel amid coronavirus outbreak \n",
      " https://edition.cnn.com/2020/03/18/football/chelsea-football-club-coronavirus-spt-intl-gbr/index.html\n",
      "\n",
      "0.8484855616189834\n",
      "Wakefield boss in hands on approach as RL helps vulnerable in coronavirus crisis \n",
      " https://www.mirror.co.uk/sport/rugby-league/wakefield-boss-takes-hands-approach-21722164\n",
      "\n",
      "0.8332894637859707\n",
      "Britain's Prince Charles tests positive for new coronavirus \n",
      " https://www.startribune.com/britain-s-prince-charles-tests-positive-for-new-coronavirus/569086732/\n",
      "\n",
      "0.8264349499136028\n",
      "Coronavirus death toll in UK rises to 137 in past 24 hours \n",
      " https://www.mirror.co.uk/news/uk-news/breaking-coronavirus-death-toll-england-21719638\n",
      "\n",
      "0.8201129013202308\n",
      "10 Cup Zojirushi NHS-18 (Uncooked) Rice Cooker $42.74 @ Amazon / Home Depot \n",
      " https://slickdeals.net/f/13891418-10-cup-zojirushi-nhs-18-uncooked-rice-cooker-42-74-amazon-home-depot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_data = reversed(sorted([(art, _y) for art, _y in zip(_final_signal, probs)], key=lambda x: x[1]))\n",
    "for art, _y in list(sorted_data)[:5]:\n",
    "    print(_y)\n",
    "    print(art['title'], \"\\n\", art['url'])    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Predictive model based on titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_title, background_title = get_data('big', field='title')  # Train\n",
    "valid_signal_title, valid_background_title = get_data('small', field='title')  # Test\n",
    "extrap_signal_title, extrap_background_title = get_data('very_small', field='title')  # Valid\n",
    "final_signal_title = tokenize(_final_signal, field='title') # Extrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented out when not optimising\n",
    "## Initially try to optimise the preprocessing\n",
    "# optimise_preprocessing(signal_title, background_title, valid_signal_title, valid_background_title,\n",
    "#                        min_df_range=(4, 14, 2), \n",
    "#                        max_df_range=(0.8, 0.96, 0.05), \n",
    "#                        ngram_range_range=(1,4,1)),\n",
    "#                        n_estimators_range=(5, 206, 20), \n",
    "#                        max_dept_range=(2, 11, 2))\n",
    "\n",
    "# Preprocess the data with discovered parameters\n",
    "vectorizer_title = TfidfVectorizer(min_df=8, max_df=0.6, ngram_range=(1,2))\n",
    "X_title = vectorizer_title.fit_transform([' '.join(doc) for doc in signal_title + background_title])\n",
    "y_title = [1]*len(signal_title) + [0]*len(background_title)\n",
    "\n",
    "X0_title = vectorizer_title.transform([' '.join(doc) for doc in valid_signal_title + valid_background_title])\n",
    "y0_title = [1]*len(valid_signal_title) + [0]*len(valid_background_title) \n",
    "\n",
    "X1_title = vectorizer_title.transform([' '.join(doc) for doc in extrap_signal_title + extrap_background_title])\n",
    "y1_title = [1]*len(extrap_signal_title) + [0]*len(extrap_background_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_title = RandomForestClassifier(n_estimators=25, max_depth=6, random_state=0, min_samples_split=2, )\n",
    "clf_title = clf.fit(X_title, y_title)\n",
    "cm_title = confusion_matrix(y1_title, clf.predict(X1_title))\n",
    "\n",
    "print(clf_title.score(X_title, y_title), clf_title.score(X0_title, y0_title), clf_title.score(X1_title, y1_title))\n",
    "print(cm_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_score = clf.score(X1, y1)\n",
    "title_score = clf_title.score(X1_title, y1_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf_title = vectorizer_title.transform([' '.join(doc) for doc in final_signal_title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.array([x[1] for x in clf.predict_proba(Xf)])\n",
    "probs_title = np.array([x[1] for x in clf_title.predict_proba(Xf_title)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Combine main and title bodies in reasonably arbitrary way\n",
    "\n",
    "The logic behind is purely anecdotal: ranking by the main body gives reasonable results, however, there are also good articles which are more difficult to correctly classify e.g. because they have very long text bodies. In these cases, the title seems to be a better indicator. In lieu of combining bodies and titles in a statistically meaningful way (which I really don't have time for), I made up a combination function in order to extract around 10 articles which have a high 'title score' and 'body score'. After this, I then revert back to the 'body' score, and select articles above a threshold picked by eye, based on the sensibleness of the articles.\n",
    "\n",
    "Not a science, but not without any logic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "def combine(score0, probs0, p0, score1, probs1, p1):\n",
    "    rank0 = percentileofscore(probs0, p0)/100\n",
    "    rank1 = score1*percentileofscore(probs1, p1)/100\n",
    "    return np.sqrt(rank0*rank0 + rank1*rank1)\n",
    "\n",
    "ranks = []\n",
    "for p0, p1 in zip(probs, probs_title):\n",
    "    rank = combine(content_score,probs, p0, title_score, probs_title, p1)\n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a good title and good content, put at the front of the list\n",
    "# Otherwise, just take articles with good content\n",
    "# Then just make an aribitrary cut-off for the pilot\n",
    "\n",
    "sorted_data = list(reversed(sorted(zip(_final_signal, ranks, probs), key=lambda x: x[1])))\n",
    "rank_sorted_data = sorted_data[:12]\n",
    "prob_sorted_data = list(reversed(sorted(sorted_data[12:], key=lambda x: x[2])))\n",
    "sorted_data = rank_sorted_data + [(art, rank, p0 ) for art, rank, p0 in prob_sorted_data if p0 >= 0.7523]\n",
    "found_titles = set()\n",
    "output = []\n",
    "for art, rank, p0 in reversed(sorted(sorted_data, key=lambda x: x[2])):\n",
    "    if art['title'] in found_titles:\n",
    "        continue\n",
    "    _art = art.copy()\n",
    "    _art['source'] = _art['source']['name']\n",
    "    output.append(_art)\n",
    "    found_titles.add(art['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output, columns=['publishedAt', 'source', 'title', 'author', 'description', 'content', 'url'])\n",
    "df.to_excel('nhsx_digital_shift_news.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main publishers in the discovery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nhs_big_search.json') as f:\n",
    "    _raw_background = json.load(f)\n",
    "with open('big_search.json') as f:\n",
    "    _raw_signal = json.load(f)\n",
    "    \n",
    "words_for_cloud = defaultdict(int)\n",
    "for term, count in Counter(row['source']['name'] for row in _raw_background + _raw_signal + _final_signal).most_common():\n",
    "    if count < 15:\n",
    "        term = 'Other'\n",
    "    words_for_cloud[term] += count\n",
    "words_for_cloud = {k: np.log(v) for k, v in words_for_cloud.items()}\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40).generate_from_frequencies(words_for_cloud)\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "ax.imshow(wordcloud, interpolation='bilinear')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
